{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's go back to the very first problem from this morning. We have an incomplete record of Newark temperatures since 1893."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "temperatures = numpy.loadtxt(\"data/newark-temperature-avg.txt\")\n",
    "temperatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But instead of analyzing it with raw Numpy, let's use Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "temperatures = pandas.Series(temperatures)\n",
    "temperatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Numpy was designed to do fast calculations with minimal dependencies.\n",
    "\n",
    "Pandas was designed to make a data analyst's life easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Behold the turning of the seasons!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can think of a Pandas Series as a Numpy array with bells and whistles, but it's more than that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It is an _indexed_ Numpy array with bells and whistles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "temperatures.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "temperatures.index = numpy.loadtxt(\"data/newark-days-ago.txt\")\n",
    "temperatures.index.name = \"days ago\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "temperatures.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "temperatures.index = pandas.to_datetime(temperatures.index, unit=\"D\", origin=pandas.Timestamp(\"2018-11-04\"))\n",
    "temperatures.index.name = \"date\"\n",
    "temperatures.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's return to the problem of imputing the missing temperature data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_temperatures = pandas.Series(numpy.loadtxt(\"data/newark-temperature-min.txt\"))\n",
    "max_temperatures = pandas.Series(numpy.loadtxt(\"data/newark-temperature-max.txt\"))\n",
    "min_temperatures.index = temperatures.index\n",
    "max_temperatures.index = temperatures.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_temperatures.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_temperatures.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It would be more convenient if these were in the same object. A DataFrame is several Series glued together with a common index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pandas.concat([temperatures, min_temperatures, max_temperatures], axis=\"columns\")\n",
    "df.columns = [\"avg\", \"min\", \"max\"]   # name them!\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Performing calculations on columns of a DataFrame is very much like Numpy: all of the elementwise operations, masking, and fancy indexing apply. In fact, Pandas usually just passes off this work on Numpy. However, it _organizes_ that work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"min-max avg\"] = (df[\"min\"] + df[\"max\"]) / 2\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Although we could have repeated the Numpy method of masking by `numpy.isnan(df[\"avg\"])`, Pandas has functions for dealing with missing data. (A lot of them, actually.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"imputed\"] = df[\"avg\"].fillna(df[\"min-max avg\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can select columns by strings in square brackets (like Numpy's record arrays, a feature I didn't show you), but rows cannot be selected by integer index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Whereas an array of length `N` is a function `[0, N) → V`, a DataFrame is a function `K → V1×V2×V3`, where `K` is the index and `V1×V2×V3` are the columns. Integer indexing won't work unless the index has integer type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    df[-1]\n",
    "except KeyError:\n",
    "    print(\"Nope, -1 is not a valid index for this DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But unlike an SQL table, a DataFrame index has an ordering. We should be able to ask for the last element, so there's a special accessor for that: `iloc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But generally, the index type should be more meaningful than the positions, so we'd want to access rows by index elements. That's `loc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[pandas.Timestamp(\"2018-11-04\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can slice regions of time just as we slice integer indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[pandas.Timestamp(\"2017-11-04\"):].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And that lets us zoom into interesting regions in the data space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"imputed\", \"avg\"]].loc[pandas.Timestamp(\"2011-01-01\"):pandas.Timestamp(\"2015-01-01\")].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pandas has oodles of statistical functions. Whenever I want to do something, I do a web search to find out what it's named— _somebody's_ asked about it on StackOverflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's look for local warming! (Not global; this is just Newark.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"imputed\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Day-to-day variations are drowning out any effect. Can we smooth these data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Yes. It's called a rolling mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"imputed\"].rolling(3*365).mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The input files we've been working with are artificial (one number per line as text). The original file that came from the NCDC was a CSV with redundant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = pandas.read_csv(\"data/newark-temperature.csv\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The first two columns have only one value because this was a database selection. Pandas's `read_csv` function recognized that the temperature columns are numbers, but not that the date is a date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df2[\"DATE\"].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The `'O'` type (object) is for cases when Pandas doesn't recognize the type of a field. In principle, it could be any Python object, but from a CSV file, it's a string.\n",
    "\n",
    "We can try to read it again, this time providing a hint that this column is supposed to be a date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df2 = pandas.read_csv(\"data/newark-temperature.csv\", parse_dates=[\"DATE\"])\n",
    "df2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It has the same columns, but now the date is a date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"DATE\"].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'd like this date column to be the index of the whole table, so we say so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.index = df2[\"DATE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now we can do the same analysis we did before, but directly on the DataFrame from the CSV file, not by gluing together Series derived from Numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"imputed\"] = df2[\"TAVG\"].fillna((df2[\"TMIN\"] + df2[\"TMAX\"]) / 2)\n",
    "df2[\"imputed\"].rolling(3*365).mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Temperature data are fairly simple: a one-dimensional time series. Let's consider something with a bit more structure— exoplanets.\n",
    "\n",
    "Each star may have a different number of planets, which complicates the indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "planets = pandas.read_csv(\"data/nasa-exoplanets.csv\")\n",
    "planets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Not knowing much about this dataset, let's get a quick summary of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "planets.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Fortunately, NASA gave us longer explanations of the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/nasa-exoplanets-details.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Number of moons? (\"`pl_mnum`: Number of Moons in System\") Awesome!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "planets[planets[\"pl_mnum\"] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "I guess not yet. They're just being hopeful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this dataset, one row is one planet. Thus, star data for stars with multiple known planets are duplicated (which effectively weights star data by their number of planets in `planets.describe()`).\n",
    "\n",
    "To get a table of stars only, we'd have to do a group-by. We expect the star data to be the same for each planet associated with a star, so the mean is an appropriate summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "planets.groupby(\"pl_hostname\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Exercise:** This reduction averages all values per star, which is not meaningful for planet variables (which start with `\"pl_\"`). After all, what does it mean to average their \"semi-major axes?\" Or their \"discovery methods?\"\n",
    "\n",
    "To avoid confusion, repeat the group-by with only the columns that start with `\"st_\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "planets.groupby(\"pl_hostname\")[[x for x in planets.columns if x.startswith(\"st_\")]].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wouldn't it be great if the index encapsulated the hierarchical relationship between stars and planets?\n",
    "\n",
    "Pandas has a `MultiIndex`, which allows sub-indexes to be nested within outer indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "planets.index = pandas.MultiIndex.from_arrays([planets[\"pl_hostname\"], planets[\"pl_letter\"]])\n",
    "planets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The transpose of this multiindexed table is a table with two levels of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "planets.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To simplify this table, let's reduce it to the following fields:\n",
    "\n",
    "  * `pl_discmethod`:  Discovery Method\n",
    "  * `pl_orbper`:      Orbital Period [days]\n",
    "  * `pl_orbsmax`:     Orbit Semi-Major Axis [AU])\n",
    "  * `pl_orbeccen`:    Eccentricity\n",
    "  * `pl_orbincl`:     Inclination [deg]\n",
    "  * `pl_bmassj`:      Planet Mass or M*sin(i) [Jupiter mass]\n",
    "  * `pl_bmassprov`:   Planet Mass or M*sin(i) Provenance\n",
    "  * `pl_disc`:        Year of Discovery\n",
    "  * `pl_telescope`:   Discovery Telescope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simple = planets[[\"pl_discmethod\", \"pl_orbper\", \"pl_orbsmax\", \"pl_orbeccen\", \"pl_bmassj\", \"pl_bmassprov\", \"pl_disc\", \"pl_telescope\"]]\n",
    "simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The syntax for selecting rows by star name now selects subtables of all planets associated with a star."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple.loc[\"tau Boo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple.loc[\"tau Cet\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How about the opposite selection? To get all planets labeled `\"b\"`— this is one planet per star. The method for that is `xs` (for \"cross-section\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simple.xs(\"b\", level=\"pl_letter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In a sense, the multiindexed table represents three dimensional information: (1) star, (2) planet letter, (3) columns. Some combinations of star and planet letter do not exist: these two dimensions are not completely filled. Some stars have only one planet, while others have as many as..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "planets[\"pl_letter\"].groupby(\"pl_hostname\").count().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...8 planets. Most have only one. I would call this dimension \"jagged\" or \"ragged.\" Pandas represents this _sparsely,_ by enumerating only the combinations that do exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Pandas used to have a type to handle 3+ dimensional data (Series is 1D, DataFrame is 2D, ...), but this was dropped in favor of multiindexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There's another library called xarray to fill this niche, but generally, I'd rather deal with 3+ dimensions sparsely than densely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we want to pass these data to a machine learning model or something, we'll have to flatten the star-planet structure.\n",
    "\n",
    "A lossless way to do that is to pivot planet letter keys into columns. (Note: the `stack` and `unstack` functions do a similar thing.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple.pivot_table(index=\"pl_hostname\", columns=\"pl_letter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There's now a column for each letter-column combination. Most of them are empty because there was one star with 8 planets but most have 1 planet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you want to simplify this table, you'll have to make choices because anything you do from here on loses information.\n",
    "\n",
    "   * Do you fill in NaN with `fillna`?\n",
    "   * Do you pick a few of the most common planet letters, like `[\"b\", \"c\", \"d\"]`?\n",
    "   * Do you average over all planets?\n",
    "   * Do you turn the original into two tables, one for each star and another for each planet, with identifiers linking them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**One last topic:** This dataset has a lot of strings. When we read it from the CSV file, Pandas left them as Python strings. This can be inefficient for very large tables (in storage and comparison speed). A common technique is to find unique strings and replace each value with an integer. This is exactly what we did with the Gettysburg Address this morning (\"dictionary encoding\"). Pandas has an automated way to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "simple[\"pl_discmethod\"].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Type `'O'` is \"object,\" for arbitrary Python objects (usually strings). We want it to be a \"category\" type. Let's convert it (`astype`) and assign it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple[\"pl_discmethod\"] = simple[\"pl_discmethod\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Uh oh... what's that warning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It's one of the most common you get with Pandas ([here's a whole blog on it](https://www.dataquest.io/blog/settingwithcopywarning/)). Remember the \"view vs copy\" discussion this morning? This is the same thing in a Pandas, rather than Numpy, context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Unlike Numpy, Pandas gives you a warning. We really should address that warning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The issue was that `simple` is a view of `planets`, so modifying a column in `simple` modifies `planets`. That could lead to surprising results.\n",
    "\n",
    "The solution? Turn `simple` into a copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple = simple.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now there's no warning because everything's fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple[\"pl_discmethod\"] = simple[\"pl_discmethod\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple[\"pl_discmethod\"].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now when we do analysis on `\"pl_discmethod\"`, it's number-crunching, rather than string-crunching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple[\"pl_discmethod\"].value_counts().plot.bar(logy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Just poking around now... discovery method by date?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple.reset_index().groupby([\"pl_disc\", \"pl_discmethod\"]).count()[\"pl_letter\"].unstack(\"pl_discmethod\").fillna(0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Wow— a lot of transits! Why is that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "planets[planets[\"pl_discmethod\"] == \"Transit\"].groupby([\"pl_disc\", \"pl_telescope\"]).count()[\"pl_letter\"].unstack(\"pl_telescope\").fillna(0).plot(figsize=(10, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Oh. Kepler. The first peak corresponds to the Kepler telescope's first mission, which ended early in instrument failure. The second mission, K2, worked around the failure to get results until earlier this year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Other than transits, what are the most popular methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "simple[simple[\"pl_discmethod\"] != \"Transit\"].reset_index().groupby([\"pl_disc\", \"pl_discmethod\"]).count()[\"pl_letter\"].unstack(\"pl_discmethod\").fillna(0).plot(figsize=(10, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Pulsar timing, apparently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "On to Dask!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
